chat_endpoint: databricks-dbrx-instruct
chat_endpoint_parameters:
  max_tokens: 500
  temperature: 0.01
chat_prompt_template: 'You are a trusted assistant that helps answer questions based
  only on the provided information. If you do not know the answer to a question, you
  truthfully say you do not know.  Here is some context which might or might not help
  you answer: {context}.  Answer directly, do not repeat the question, do not start
  with something like: the answer to the question, do not add AI in front of your
  answer, do not say: here is the answer, do not mention the context or the question.
  Based on this context, answer this question: {question}'
chat_prompt_template_variables:
- context
- question
chunk_template: '`{chunk_text}`

  '
data_pipeline_config:
  chunking_strategy:
    default: "RecursiveTextSplitterByTokens(embedding_model_name=BAAI/bge-large-en-v1.5,\
      \ chunk_size_tokens=400, chunk_overlap_tokens=100, embedding_model_config={'context_window':\
      \ 512, 'tokenizer': 'hugging_face', 'type': 'FMAPI'}, tokenizer=BertTokenizerFast(name_or_path='BAAI/bge-large-en-v1.5',\
      \ vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right',\
      \ truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token':\
      \ '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'},\
      \ clean_up_tokenization_spaces=True),  added_tokens_decoder={\n\t0: AddedToken(\"\
      [PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\
      \t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False,\
      \ normalized=False, special=True),\n\t101: AddedToken(\"[CLS]\", rstrip=False,\
      \ lstrip=False, single_word=False, normalized=False, special=True),\n\t102:\
      \ AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False,\
      \ special=True),\n\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False,\
      \ single_word=False, normalized=False, special=True),\n}, text_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter\
      \ object at 0x7f2142d77e80>)"
    md: MarkdownHeaderSplitter(headers_to_split_on=[('#', 'Header 1'), ('##', 'Header
      2'), ('###', 'Header 3')], include_headers_in_chunks=True, text_splitter=<langchain_text_splitters.markdown.MarkdownHeaderTextSplitter
      object at 0x7f213fa5e110>)
  embedding_model:
    endpoint: databricks-bge-large-en
    model_name: BAAI/bge-large-en-v1.5
  parsing_strategy:
    docx: PyPandocDocx()
    html: HTMLToMarkdownify()
    md: PassThroughNoParsing()
    pdf: UnstructuredPDF(strategy=fast, hi_res_model_name=yolox)
    pptx: UnstructuredPPTX()
  tag: bge_test_1
vector_search_endpoint_name: sunish_rag_vs_endpoint
vector_search_index: ep_05_08_release.rag.pdf_docs__bge_test_1__gold_chunked_index
vector_search_parameters:
  k: 3
vector_search_schema:
  chunk_text: chunk_text
  document_source: doc_uri
  primary_key: chunk_id
